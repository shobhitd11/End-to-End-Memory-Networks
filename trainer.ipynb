{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train.pickle', 'rb') as handle:\n",
    "     train = pickle.load(handle)\n",
    "with open('test.pickle', 'rb') as handle:\n",
    "     test = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for the first task, extend the same to the other tasks as well\n",
    "#adjacent weight sharing\n",
    "\n",
    "HOPS=3\n",
    "V = -1 #find the vocabulary size\n",
    "maxlen = -1 #find the maximum length of a sentence\n",
    "vocab = {} #find the distinct number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the training data\n",
    "import re\n",
    "punctuation = re.compile(r'[-.?!,\":;()|0-9]')\n",
    "for idx in range(20):\n",
    "    for key,data in zip(train[idx+1].keys(),train[idx+1].values()):\n",
    "        memories = data['memory']\n",
    "\n",
    "        query = data['query'].strip('\\n')\n",
    "        data['query'] = punctuation.sub(\"\", query).strip().lower().split()\n",
    "        maxlen = max(maxlen,len(data['query']))\n",
    "        for word in data['query']:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)+1\n",
    "\n",
    "        answer = data['answer'].strip('\\n')\n",
    "        data['answer'] = punctuation.sub(\"\", answer).strip().lower().split()\n",
    "        maxlen = max(maxlen,len(data['answer']))\n",
    "        for word in data['answer']:\n",
    "\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)+1\n",
    "\n",
    "        data['fact'] = data['fact'].strip('\\n').strip()\n",
    "\n",
    "        for i in range(len(memories)):\n",
    "            memories[i] = memories[i].strip('\\n')\n",
    "            memories[i] = punctuation.sub(\"\", memories[i])\n",
    "            memories[i] = memories[i].strip().lower().split()\n",
    "            maxlen = max(maxlen,len(memories[i]))\n",
    "\n",
    "            for word in memories[i]:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)+1\n",
    "\n",
    "        data['memory'] = memories\n",
    "        train[idx+1][key] = data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the testing data\n",
    "import re\n",
    "punctuation = re.compile(r'[-.?!,\":;()|0-9]')\n",
    "for idx in range(20):\n",
    "    for key,data in zip(test[idx+1].keys(),test[idx+1].values()):\n",
    "        memories = data['memory']\n",
    "\n",
    "        query = data['query'].strip('\\n')\n",
    "        data['query'] = punctuation.sub(\"\", query).strip().lower().split()\n",
    "        maxlen = max(maxlen,len(data['query']))\n",
    "        for word in data['query']:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)+1\n",
    "\n",
    "        answer = data['answer'].strip('\\n')\n",
    "        data['answer'] = punctuation.sub(\"\", answer).strip().lower().split()\n",
    "        maxlen = max(maxlen,len(data['answer']))\n",
    "        for word in data['answer']:\n",
    "\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)+1\n",
    "\n",
    "        data['fact'] = data['fact'].strip('\\n').strip()\n",
    "\n",
    "        for i in range(len(memories)):\n",
    "            memories[i] = memories[i].strip('\\n')\n",
    "            memories[i] = punctuation.sub(\"\", memories[i])\n",
    "            memories[i] = memories[i].strip().lower().split()\n",
    "            maxlen = max(maxlen,len(memories[i]))\n",
    "\n",
    "            for word in memories[i]:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)+1\n",
    "\n",
    "        data['memory'] = memories\n",
    "        test[idx+1][key] = data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['<PAD>'] = 0\n",
    "V = len(vocab)\n",
    "d = 50\n",
    "H=3\n",
    "RN=True\n",
    "MEM_SIZE = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch . nn as nn\n",
    "import torch . nn . functional as F\n",
    "import torch.autograd as ag\n",
    "import numpy as np\n",
    "# This is our neural networks class\n",
    "class MemN2N ( nn . Module ):\n",
    "# Here we define our netwrok structure\n",
    "    \n",
    "    def getGaussianMatrix(self,Vo,dim):\n",
    "        mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "        s = np.random.normal(mu, sigma, Vo*dim)\n",
    "        s = s.reshape(Vo,-1)\n",
    "        s[0] = 0\n",
    "        return s\n",
    "    \n",
    "    def getGaussianMatrix3D(self,x,y,z):\n",
    "        mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "        s = np.random.normal(mu, sigma, x*y*z)\n",
    "        s = s.reshape(x,y,z)\n",
    "        return s\n",
    "\n",
    "    def constrainPadding(self):\n",
    "        self.C0.weight.data[0] = 0\n",
    "        self.C1.weight.data[0] = 0\n",
    "        self.C2.weight.data[0] = 0\n",
    "        self.C3.weight.data[0] = 0\n",
    "\n",
    "        \n",
    "    def __init__ ( self, vocab_size = V, embedding_dim = (V,d), hops = H):\n",
    "        super ( MemN2N,self ). __init__ ()\n",
    "        self.MEM_SIZE = MEM_SIZE\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.RN = RN\n",
    "        self.d = d\n",
    "        self.maxlen = maxlen\n",
    "#         self.memory_size = ag.Variable(torch.zeros(BATCH_SIZE).int().cuda(),requires_grad=False)\n",
    "        self.L = np.zeros((maxlen,d))\n",
    "        self.linear_mode = True\n",
    "        for k in range(self.L.shape[1]-1):\n",
    "            for j in range(self.L.shape[0]-1):\n",
    "                self.L[j,k] = (1-float(j+1)/self.L.shape[0]) - ((k+1)/self.L.shape[1])*(1-(2*float(j+1))/self.L.shape[0])\n",
    "        if torch.cuda.is_available():\n",
    "            self.L = ag.Variable(torch.from_numpy(self.L).contiguous().cuda(),requires_grad=False)\n",
    "        else:\n",
    "            self.L = ag.Variable(torch.from_numpy(self.L).contiguous(),requires_grad=False)\n",
    "\n",
    "#         self.memories = ag.Variable(torch.zeros(BATCH_SIZE,MEM_SIZE,d).double().cuda(),requires_grad=False)\n",
    "#         self.output_memories = ag.Variable(torch.zeros(BATCH_SIZE,MEM_SIZE,d).double().cuda(),requires_grad=False)\n",
    "        self.V = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hops = hops\n",
    "        self.C0 = nn . Embedding (embedding_dim[0] , embedding_dim[1], padding_idx=0).double()\n",
    "        self.C1 = nn . Embedding (embedding_dim[0] , embedding_dim[1], padding_idx=0).double()\n",
    "        self.C2 = nn . Embedding (embedding_dim[0] , embedding_dim[1], padding_idx=0).double()\n",
    "        self.C3 = nn . Embedding (embedding_dim[0] , embedding_dim[1], padding_idx=0).double()\n",
    "        \n",
    "        #initialize random weight based on Gaussian Distribution\n",
    "        \n",
    "        self.C0.weight.data.copy_(torch.from_numpy(self.getGaussianMatrix(self.V,self.d)))\n",
    "        self.C1.weight.data.copy_(torch.from_numpy(self.getGaussianMatrix(self.V,self.d)))\n",
    "        self.C2.weight.data.copy_(torch.from_numpy(self.getGaussianMatrix(self.V,self.d)))\n",
    "        self.C3.weight.data.copy_(torch.from_numpy(self.getGaussianMatrix(self.V,self.d)))\n",
    "\n",
    "        self.T_C0 = nn . Embedding (self.MEM_SIZE, embedding_dim[1], padding_idx=0).double()\n",
    "        self.T_C1 = nn . Embedding (self.MEM_SIZE, embedding_dim[1], padding_idx=0).double()\n",
    "        self.T_C2 = nn . Embedding (self.MEM_SIZE , embedding_dim[1], padding_idx=0).double()\n",
    "        self.T_C3 = nn . Embedding (self.MEM_SIZE , embedding_dim[1], padding_idx=0).double()\n",
    "        \n",
    "        #initialize random weight based on Gaussian Distribution\n",
    "        \n",
    "        self.T_C0.weight.data.copy_(torch.from_numpy(self.getGaussianMatrix(self.MEM_SIZE,self.d)))\n",
    "        self.T_C1.weight.data.copy_(torch.from_numpy(self.getGaussianMatrix(self.MEM_SIZE,self.d)))\n",
    "        self.T_C2.weight.data.copy_(torch.from_numpy(self.getGaussianMatrix(self.MEM_SIZE,self.d)))\n",
    "        self.T_C3.weight.data.copy_(torch.from_numpy(self.getGaussianMatrix(self.MEM_SIZE,self.d)))\n",
    "\n",
    "        \n",
    "        #initialize zero weight to padded input\n",
    "        \n",
    "        #initialize zero weight to padded input\n",
    "#         self.constrainPadding()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.C0 = self.C0.cuda()\n",
    "            self.C1 = self.C1.cuda()\n",
    "            self.C2 = self.C2.cuda()\n",
    "            self.C3 = self.C3.cuda()\n",
    "            self.T_C0 = self.T_C0.cuda()\n",
    "            self.T_C1 = self.T_C1.cuda()\n",
    "            self.T_C2 = self.T_C2.cuda()\n",
    "            self.T_C3 = self.T_C3.cuda()\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        self.memories = None\n",
    "        self.output_memories = None\n",
    "        \n",
    "        \n",
    "    def forward ( self , x , q ,temp):\n",
    "            \n",
    "        A_k = self.C0\n",
    "        B_k = self.C0\n",
    "        C_k = self.C1\n",
    "        \n",
    "        A_k_T = self.T_C0\n",
    "        C_k_T = self.T_C1\n",
    "            \n",
    "        \n",
    "        mat = self.L.unsqueeze(0).repeat(self.BATCH_SIZE,1,1)\n",
    "#         mat = self.L.repeat(self.BATCH_SIZE)\n",
    "        Q = B_k(q)*mat\n",
    "\n",
    "#         print(weighted_Q.view(-1,d,d))\n",
    "        Q = torch.sum(Q,dim=1)\n",
    "        mat1 = self.L.unsqueeze(0).repeat(self.BATCH_SIZE*self.MEM_SIZE,1,1)\n",
    "        x = x.view(-1,self.maxlen)\n",
    "        \n",
    "        for i in range(self.hops):\n",
    "            self.reset_memory()\n",
    "            C_k = getattr(self, 'C'+str(i+1))\n",
    "            C_k_T = getattr(self, 'T_C'+str(i+1))\n",
    "            \n",
    "#             print(x.size())\n",
    "            \n",
    "#             mat = self.L.repeat(self.BATCH_SIZE,self.MEM_SIZE)\n",
    "            weighted_M = (A_k(x)*mat1).view(self.BATCH_SIZE,self.MEM_SIZE,self.maxlen,-1)\n",
    "            weighted_O = (C_k(x)*mat1).view(self.BATCH_SIZE,self.MEM_SIZE,self.maxlen,-1)\n",
    "            sum_M = torch.sum(weighted_M,dim=2)\n",
    "            sum_O = torch.sum(weighted_O,dim=2)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                temporal_encoding = ag.Variable(torch.from_numpy(temp).long().cuda())\n",
    "            else:\n",
    "                temporal_encoding = ag.Variable(torch.from_numpy(temp).long())\n",
    "\n",
    "            self.memories = sum_M + A_k_T(temporal_encoding)\n",
    "            self.output_memories = sum_O + C_k_T(temporal_encoding)\n",
    "            \n",
    "            \n",
    "#             if(RN == True):\n",
    "#                 s1 = self.getGaussianMatrix3D(self.BATCH_SIZE,(int(self.MEM_SIZE/10)),self.d)\n",
    "#                 s2 = self.getGaussianMatrix3D(self.BATCH_SIZE,(int(self.MEM_SIZE/10)),self.d)\n",
    "#                 if torch.cuda.is_available():\n",
    "#                     noise1 = ag.Variable(torch.from_numpy(s1).double().cuda())    \n",
    "#                     noise2 = ag.Variable(torch.from_numpy(s2).double().cuda())    \n",
    "#                 else:\n",
    "#                     noise1 = ag.Variable(torch.from_numpy(s1).double())    \n",
    "#                     noise2 = ag.Variable(torch.from_numpy(s2).double())    \n",
    "                \n",
    "#                 self.memories = torch.cat((self.memories,noise1),dim=1)\n",
    "#                 self.output_memories = torch.cat((self.output_memories,noise2),dim=1)\n",
    "                \n",
    "#                 self.MEM_SIZE = int(1.1*self.MEM_SIZE)\n",
    "            \n",
    "            product = torch.bmm(Q.unsqueeze(1).repeat(1,self.MEM_SIZE,1).unsqueeze(3).permute(0,1,3,2).view(-1,1,d),self.memories.unsqueeze(3).view(-1,self.d,1)) # batch size x 50\n",
    "#           print(product.size())\n",
    "            product = product.view(self.BATCH_SIZE,self.MEM_SIZE,1).squeeze(2)\n",
    "            if(self.linear_mode == False):\n",
    "                product = F.softmax(product, dim=1) # batch size x 50\n",
    "#             print(product[1])\n",
    "            product = product.unsqueeze(2).repeat(1,1,self.d)\n",
    "            O_k = torch.sum(product*self.output_memories, dim = 1)\n",
    "#             print(O_k.shape)\n",
    "#             print(Q.shape)\n",
    "            Q = Q + O_k #batch size x d\n",
    "            A_k = C_k\n",
    "            A_k_T = C_k_T\n",
    "            \n",
    "#         self.constrainPadding()\n",
    "        output = torch.mm(Q,C_k.weight.permute(1,0))\n",
    "        values, indices = torch.max(output,1)\n",
    "#         print(indices)\n",
    "        return output\n",
    "                \n",
    "    # This is a helper function to understand the dimensions\n",
    "    def num_flat_features ( self , x ):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        return np.prod(size)\n",
    "\n",
    "def network():\n",
    "    net = MemN2N ()\n",
    "    net.hops = 3\n",
    "    print ( net )\n",
    "    # for name,param in list(net.named_parameters()):\n",
    "    #     if param.requires_grad:\n",
    "    #         print name, param.data\n",
    "    if torch.cuda.is_available():\n",
    "        net = net.cuda()\n",
    "    return net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemN2N(\n",
      "  (C0): Embedding(175, 50, padding_idx=0)\n",
      "  (C1): Embedding(175, 50, padding_idx=0)\n",
      "  (C2): Embedding(175, 50, padding_idx=0)\n",
      "  (C3): Embedding(175, 50, padding_idx=0)\n",
      "  (T_C0): Embedding(50, 50, padding_idx=0)\n",
      "  (T_C1): Embedding(50, 50, padding_idx=0)\n",
      "  (T_C2): Embedding(50, 50, padding_idx=0)\n",
      "  (T_C3): Embedding(50, 50, padding_idx=0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000\n",
    "NUM_BATCHES = (NUM_SAMPLES+BATCH_SIZE-1)/BATCH_SIZE\n",
    "gamma = 0.01\n",
    "gamma_i = 0.005\n",
    "NUM_EPOCHS = 200\n",
    "optimizer = torch . optim . SGD ( net . parameters () , lr = gamma_i)\n",
    "scheduler = torch . optim . lr_scheduler . StepLR(optimizer, step_size=NUM_EPOCHS/4, gamma=1)\n",
    "clip = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strictly_decreasing(L):\n",
    "    return all(x<=y for x, y in zip(L, L[1:]))\n",
    "\n",
    "def trainer():\n",
    "    if torch.cuda.is_available():\n",
    "        criterion = nn . CrossEntropyLoss ().cuda()\n",
    "    else:\n",
    "        criterion = nn . CrossEntropyLoss ()\n",
    "\n",
    "    NUM_SAMPLES = 1000\n",
    "    NUM_BATCHES = (NUM_SAMPLES+BATCH_SIZE-1)/BATCH_SIZE\n",
    "    gamma = 0.1\n",
    "    gamma_i = 0.005\n",
    "    NUM_EPOCHS = 300\n",
    "    optimizer = torch . optim . SGD ( net . parameters () , lr = gamma_i)\n",
    "    scheduler = torch . optim . lr_scheduler . StepLR(optimizer, step_size=NUM_EPOCHS/4, gamma=1)\n",
    "    clip = 40\n",
    "\n",
    "    task_perm = np.random.permutation(20)\n",
    "    # task_perm = [1]\n",
    "    net.linear_mode = True\n",
    "    flag = True\n",
    "    loss_arr = []\n",
    "    val_arr = []\n",
    "    length = dict()\n",
    "    nptasks = dict()\n",
    "    training_data = dict()\n",
    "    validation_data = dict()\n",
    "    train_perm = dict()\n",
    "    valid_perm = dict()\n",
    "    batch_perm = dict()\n",
    "    memories = dict()\n",
    "    memories_valid = dict()\n",
    "\n",
    "    batch_perm_valid = dict()\n",
    "    # for task in task_perm:\n",
    "    #     nptasks[task] = train[task+1].values()\n",
    "    #     length[task] = len(nptasks[task]) \n",
    "    #     perm = np.arange(min(NUM_SAMPLES,length[task]))\n",
    "    #     train_perm[task] = perm[:int(0.9*min(NUM_SAMPLES,length[task]))]\n",
    "    #     valid_perm[task] = perm[int(0.9*min(NUM_SAMPLES,length[task])):]\n",
    "    #     training_data[task] = train[task+1].values()[:int(0.9*min(NUM_SAMPLES,length[task]))]\n",
    "    #     validation_data[task] = nptasks[task][int(0.9*min(NUM_SAMPLES,length[task])):]\n",
    "\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):  \n",
    "\n",
    "        task_perm = np.random.permutation(20)\n",
    "\n",
    "        for task in task_perm:\n",
    "    #         batch_perm[task] = np.arange(len(training_data[task]))\n",
    "    #         batch_perm_valid[task] = np.arange(len(validation_data[task]))\n",
    "            memories[task] = []\n",
    "            memories_valid[task] = []\n",
    "\n",
    "\n",
    "        running_loss = 0\n",
    "        running_valid = 0\n",
    "        scheduler.step()\n",
    "\n",
    "        for i in range(int(NUM_BATCHES*0.9)):\n",
    "\n",
    "            for task_i in task_perm:\n",
    "\n",
    "\n",
    "    #             batch_perm_t = batch_perm[task_i][BATCH_SIZE*i:BATCH_SIZE*(i+1)]\n",
    "                training_batch = train[task_i+1].values()[:int(0.9*min(NUM_SAMPLES,len(train[task_i+1].values())))][BATCH_SIZE*i:BATCH_SIZE*(i+1)]\n",
    "\n",
    "\n",
    "                inputs = np.zeros((BATCH_SIZE,MEM_SIZE,maxlen))\n",
    "\n",
    "                queries = np.zeros((BATCH_SIZE,maxlen))\n",
    "\n",
    "                outputs = np.zeros((BATCH_SIZE))\n",
    "\n",
    "\n",
    "                temporal_encoding = np.zeros((BATCH_SIZE,MEM_SIZE))\n",
    "                for batch,batch_i in zip(training_batch,range(BATCH_SIZE)):\n",
    "                    for memory,mem_i in zip(batch['memory'],range(len(batch['memory']))):\n",
    "                        memories[task_i].append(memory)\n",
    "                        if(len(memories[task_i])>MEM_SIZE):\n",
    "                            memories[task_i].pop(0)\n",
    "\n",
    "                    for memory,mem_i in zip(reversed(memories[task_i]),range(min(len(memories[task_i]),45))):\n",
    "                        for memory_word,word_i in zip(memory,range(len(memory))):\n",
    "                            inputs[batch_i,mem_i,word_i] = vocab[memory_word]\n",
    "                        temporal_encoding[batch_i,mem_i] = mem_i+1\n",
    "\n",
    "                    for query_word,query_word_i in zip(batch['query'],range(len(batch['query']))):\n",
    "                            queries[batch_i,query_word_i] = vocab[query_word]\n",
    "                    outputs[batch_i] = vocab[batch['answer'][0]]\n",
    "\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = ag.Variable(torch.from_numpy(inputs).long().cuda())\n",
    "                    queries = ag.Variable(torch.from_numpy(queries).long().cuda())\n",
    "                    outputs = ag.Variable(torch.from_numpy(outputs).long().cuda())\n",
    "                else:\n",
    "                    inputs = ag.Variable(torch.from_numpy(inputs).long())\n",
    "                    queries = ag.Variable(torch.from_numpy(queries).long())\n",
    "                    outputs = ag.Variable(torch.from_numpy(outputs).long())\n",
    "\n",
    "\n",
    "\n",
    "                net.zero_grad()\n",
    "                op = net(inputs,queries,temporal_encoding)\n",
    "#                 print(outputs)\n",
    "#                 print(batch)\n",
    "#                 print(inputs)\n",
    "                loss = criterion(op,outputs).cuda()\n",
    "                loss.backward()\n",
    "                for name,param in list(net.named_parameters()):\n",
    "                    if param.requires_grad :\n",
    "                        param.grad[0] = 0\n",
    "\n",
    "\n",
    "    #             for p in net.parameters():\n",
    "    #                 param_norm = p.grad.data.norm(2)\n",
    "    #                 if(param_norm > clip):\n",
    "    #                     p.grad.data.mul_(clip/param_norm)\n",
    "\n",
    "                nn.utils.clip_grad_norm(net.parameters(), clip, 2)\n",
    "    #             for p in net.parameters():\n",
    "    #                 param_norm = p.grad.data.norm(2)\n",
    "    #                 if(param_norm > clip):\n",
    "    #                     p.grad.data.mul_(clip/param_norm)\n",
    "\n",
    "\n",
    "                optimizer.step()\n",
    "                running_loss += loss[0]\n",
    "    #         if i % 20 == 19:\n",
    "        print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss/(20*len(task_perm))))\n",
    "        loss_arr.append(running_loss)\n",
    "        \n",
    "        for i in range(int(0.9*NUM_BATCHES),NUM_BATCHES):\n",
    "\n",
    "            for task_i in task_perm:\n",
    "\n",
    "                valid_start = BATCH_SIZE*i\n",
    "                valid_end = BATCH_SIZE*(i+1)\n",
    "\n",
    "    #             batch_perm_valid_t = batch_perm_valid[task_i][valid_start:valid_end]\n",
    "    #             print(batch_perm_valid_t)\n",
    "\n",
    "                validation_batch = train[task_i+1].values()[valid_start:valid_end]\n",
    "\n",
    "\n",
    "\n",
    "                inputs = np.zeros((BATCH_SIZE,MEM_SIZE,maxlen))\n",
    "\n",
    "                queries = np.zeros((BATCH_SIZE,maxlen))\n",
    "\n",
    "                outputs = np.zeros((BATCH_SIZE))\n",
    "\n",
    "\n",
    "                temporal_encoding = np.zeros((BATCH_SIZE,MEM_SIZE))\n",
    "                for batch,batch_i in zip(validation_batch,range(BATCH_SIZE)):\n",
    "                    for memory,mem_i in zip(batch['memory'],range(len(batch['memory']))):\n",
    "                        memories_valid[task_i].append(memory)\n",
    "                        if(len(memories_valid[task_i])>MEM_SIZE):\n",
    "                            memories_valid[task_i].pop(0)\n",
    "\n",
    "                    for memory,mem_i in zip(reversed(memories_valid[task_i]),range(min(len(memories_valid[task_i]),45))):\n",
    "                        for memory_word,word_i in zip(memory,range(len(memory))):\n",
    "                            inputs[batch_i,mem_i,word_i] = vocab[memory_word]\n",
    "                        temporal_encoding[batch_i,mem_i] = mem_i+1\n",
    "\n",
    "                    for query_word,query_word_i in zip(batch['query'],range(len(batch['query']))):\n",
    "                            queries[batch_i,query_word_i] = vocab[query_word]\n",
    "                    outputs[batch_i] = vocab[batch['answer'][0]]\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = ag.Variable(torch.from_numpy(inputs).long().cuda())\n",
    "                    queries = ag.Variable(torch.from_numpy(queries).long().cuda())\n",
    "                    outputs = ag.Variable(torch.from_numpy(outputs).long().cuda())\n",
    "                else:\n",
    "                    inputs = ag.Variable(torch.from_numpy(inputs).long())\n",
    "                    queries = ag.Variable(torch.from_numpy(queries).long())\n",
    "                    outputs = ag.Variable(torch.from_numpy(outputs).long())\n",
    "\n",
    "\n",
    "                op1 = net(inputs,queries,temporal_encoding)\n",
    "                loss1 = criterion(op1,outputs).cuda()\n",
    "\n",
    "                running_valid += loss1[0]\n",
    "\n",
    "    #         if i % 20 == 19:\n",
    "        print('[%d, %5d] valid loss: %.3f' % (epoch + 1, i + 1, running_valid))\n",
    "        val_arr.append(round(float(running_valid[0].data),5))\n",
    "\n",
    "        if(flag == True):\n",
    "            if len(val_arr)>=2 and strictly_decreasing(val_arr[-2:]):\n",
    "                net.linear_mode = False\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = gamma\n",
    "                    scheduler.gamma = 0.5\n",
    "                    print 'Linear Start Complete'\n",
    "                    flag = False\n",
    "    \n",
    "    return val_arr,loss_arr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_func(task):\n",
    "    accuracy = []\n",
    "    testing_data = np.array(test[task+1].values())\n",
    "    length = len(testing_data) \n",
    "    memories = []\n",
    "    for i in range(min(NUM_BATCHES,(length+BATCH_SIZE-1)/BATCH_SIZE)):\n",
    "        testing_batch = testing_data[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "    #             print(testing_batch)\n",
    "\n",
    "        inputs = np.zeros((BATCH_SIZE,MEM_SIZE,maxlen))\n",
    "\n",
    "        queries = np.zeros((BATCH_SIZE,maxlen))\n",
    "\n",
    "        outputs = np.zeros((BATCH_SIZE))\n",
    "\n",
    "\n",
    "        temporal_encoding = np.zeros((BATCH_SIZE,MEM_SIZE))\n",
    "        for batch,batch_i in zip(testing_batch,range(BATCH_SIZE)):\n",
    "            for memory,mem_i in zip(batch['memory'],range(len(batch['memory']))):\n",
    "                memories.append(memory)\n",
    "                if(len(memories)>MEM_SIZE):\n",
    "                    memories.pop(0)\n",
    "\n",
    "            for memory,mem_i in zip(reversed(memories),range(min(len(memories),45))):\n",
    "                for memory_word,word_i in zip(memory,range(len(memory))):\n",
    "                    inputs[batch_i,mem_i,word_i] = vocab[memory_word]\n",
    "                temporal_encoding[batch_i,mem_i] = mem_i+1\n",
    "\n",
    "            for query_word,query_word_i in zip(batch['query'],range(len(batch['query']))):\n",
    "                    queries[batch_i,query_word_i] = vocab[query_word]\n",
    "            outputs[batch_i] = vocab[batch['answer'][0]]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = ag.Variable(torch.from_numpy(inputs).long().cuda())\n",
    "            queries = ag.Variable(torch.from_numpy(queries).long().cuda())\n",
    "            outputs = ag.Variable(torch.from_numpy(outputs).long().cuda())\n",
    "        else:\n",
    "            inputs = ag.Variable(torch.from_numpy(inputs).long())\n",
    "            queries = ag.Variable(torch.from_numpy(queries).long())\n",
    "            outputs = ag.Variable(torch.from_numpy(outputs).long())\n",
    "\n",
    "\n",
    "\n",
    "        op_test = net(inputs,queries,temporal_encoding)\n",
    "        acc = np.mean(np.argmax(op_test.data.cpu().numpy(),axis=1) == outputs.data.cpu().numpy())\n",
    "\n",
    "        accuracy.append(acc)\n",
    "    return np.mean(accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemN2N(\n",
      "  (C0): Embedding(175, 50, padding_idx=0)\n",
      "  (C1): Embedding(175, 50, padding_idx=0)\n",
      "  (C2): Embedding(175, 50, padding_idx=0)\n",
      "  (C3): Embedding(175, 50, padding_idx=0)\n",
      "  (T_C0): Embedding(50, 50, padding_idx=0)\n",
      "  (T_C1): Embedding(50, 50, padding_idx=0)\n",
      "  (T_C2): Embedding(50, 50, padding_idx=0)\n",
      "  (T_C3): Embedding(50, 50, padding_idx=0)\n",
      ")\n",
      "[1,    28] loss: 457.015\n",
      "[1,    32] valid loss: 289.188\n",
      "[2,    28] loss: 4.865\n",
      "[2,    32] valid loss: 245.544\n",
      "[3,    28] loss: 3.643\n",
      "[3,    32] valid loss: 236.084\n",
      "[4,    28] loss: 3.384\n",
      "[4,    32] valid loss: 236.603\n",
      "Linear Start Complete\n",
      "[5,    28] loss: 6.561\n",
      "[5,    32] valid loss: 348.083\n",
      "[6,    28] loss: 5.070\n",
      "[6,    32] valid loss: 298.586\n",
      "[7,    28] loss: 4.492\n",
      "[7,    32] valid loss: 283.310\n",
      "[8,    28] loss: 4.211\n",
      "[8,    32] valid loss: 272.223\n",
      "[9,    28] loss: 3.992\n",
      "[9,    32] valid loss: 263.142\n",
      "[10,    28] loss: 3.809\n",
      "[10,    32] valid loss: 255.342\n",
      "[11,    28] loss: 3.652\n",
      "[11,    32] valid loss: 248.749\n",
      "[12,    28] loss: 3.520\n",
      "[12,    32] valid loss: 243.331\n",
      "[13,    28] loss: 3.412\n",
      "[13,    32] valid loss: 238.900\n",
      "[14,    28] loss: 3.321\n",
      "[14,    32] valid loss: 235.171\n",
      "[15,    28] loss: 3.243\n",
      "[15,    32] valid loss: 231.948\n",
      "[16,    28] loss: 3.175\n",
      "[16,    32] valid loss: 229.146\n",
      "[17,    28] loss: 3.116\n",
      "[17,    32] valid loss: 226.741\n",
      "[18,    28] loss: 3.065\n",
      "[18,    32] valid loss: 224.687\n",
      "[19,    28] loss: 3.021\n",
      "[19,    32] valid loss: 222.955\n",
      "[20,    28] loss: 2.984\n",
      "[20,    32] valid loss: 221.511\n",
      "[21,    28] loss: 2.953\n",
      "[21,    32] valid loss: 220.306\n",
      "[22,    28] loss: 2.927\n",
      "[22,    32] valid loss: 219.299\n",
      "[23,    28] loss: 2.904\n",
      "[23,    32] valid loss: 218.473\n",
      "[24,    28] loss: 2.884\n",
      "[24,    32] valid loss: 217.757\n",
      "[25,    28] loss: 2.867\n",
      "[25,    32] valid loss: 217.170\n",
      "[26,    28] loss: 2.852\n",
      "[26,    32] valid loss: 216.656\n",
      "[27,    28] loss: 2.838\n",
      "[27,    32] valid loss: 216.220\n",
      "[28,    28] loss: 2.826\n",
      "[28,    32] valid loss: 215.823\n",
      "[29,    28] loss: 2.815\n",
      "[29,    32] valid loss: 215.470\n",
      "[30,    28] loss: 2.805\n",
      "[30,    32] valid loss: 215.148\n",
      "[31,    28] loss: 2.796\n",
      "[31,    32] valid loss: 214.844\n",
      "[32,    28] loss: 2.788\n",
      "[32,    32] valid loss: 214.552\n",
      "[33,    28] loss: 2.780\n",
      "[33,    32] valid loss: 214.275\n",
      "[34,    28] loss: 2.772\n",
      "[34,    32] valid loss: 214.005\n",
      "[35,    28] loss: 2.764\n",
      "[35,    32] valid loss: 213.719\n",
      "[36,    28] loss: 2.757\n",
      "[36,    32] valid loss: 213.456\n",
      "[37,    28] loss: 2.750\n",
      "[37,    32] valid loss: 213.186\n",
      "[38,    28] loss: 2.743\n",
      "[38,    32] valid loss: 212.892\n",
      "[39,    28] loss: 2.736\n",
      "[39,    32] valid loss: 212.592\n",
      "[40,    28] loss: 2.728\n",
      "[40,    32] valid loss: 212.266\n",
      "[41,    28] loss: 2.720\n",
      "[41,    32] valid loss: 211.904\n",
      "[42,    28] loss: 2.711\n",
      "[42,    32] valid loss: 211.491\n",
      "[43,    28] loss: 2.701\n",
      "[43,    32] valid loss: 211.028\n",
      "[44,    28] loss: 2.691\n",
      "[44,    32] valid loss: 210.590\n",
      "[45,    28] loss: 2.681\n",
      "[45,    32] valid loss: 210.147\n",
      "[46,    28] loss: 2.671\n",
      "[46,    32] valid loss: 209.701\n",
      "[47,    28] loss: 2.661\n",
      "[47,    32] valid loss: 209.252\n",
      "[48,    28] loss: 2.651\n",
      "[48,    32] valid loss: 208.804\n",
      "[49,    28] loss: 2.640\n",
      "[49,    32] valid loss: 208.341\n",
      "[50,    28] loss: 2.630\n",
      "[50,    32] valid loss: 207.873\n",
      "[51,    28] loss: 2.619\n",
      "[51,    32] valid loss: 207.356\n",
      "[52,    28] loss: 2.608\n",
      "[52,    32] valid loss: 206.835\n",
      "[53,    28] loss: 2.596\n",
      "[53,    32] valid loss: 206.251\n",
      "[54,    28] loss: 2.584\n",
      "[54,    32] valid loss: 205.646\n",
      "[55,    28] loss: 2.572\n",
      "[55,    32] valid loss: 205.062\n",
      "[56,    28] loss: 2.561\n",
      "[56,    32] valid loss: 204.483\n",
      "[57,    28] loss: 2.549\n",
      "[57,    32] valid loss: 203.937\n",
      "[58,    28] loss: 2.538\n",
      "[58,    32] valid loss: 203.419\n",
      "[59,    28] loss: 2.527\n",
      "[59,    32] valid loss: 202.902\n",
      "[60,    28] loss: 2.517\n",
      "[60,    32] valid loss: 202.405\n",
      "[61,    28] loss: 2.506\n",
      "[61,    32] valid loss: 201.930\n",
      "[62,    28] loss: 2.496\n",
      "[62,    32] valid loss: 201.455\n",
      "[63,    28] loss: 2.486\n",
      "[63,    32] valid loss: 201.016\n",
      "[64,    28] loss: 2.476\n",
      "[64,    32] valid loss: 200.577\n",
      "[65,    28] loss: 2.466\n",
      "[65,    32] valid loss: 200.158\n",
      "[66,    28] loss: 2.457\n",
      "[66,    32] valid loss: 199.750\n",
      "[67,    28] loss: 2.447\n",
      "[67,    32] valid loss: 199.348\n",
      "[68,    28] loss: 2.438\n",
      "[68,    32] valid loss: 198.947\n",
      "[69,    28] loss: 2.429\n",
      "[69,    32] valid loss: 198.582\n",
      "[70,    28] loss: 2.421\n",
      "[70,    32] valid loss: 198.206\n",
      "[71,    28] loss: 2.412\n",
      "[71,    32] valid loss: 197.859\n",
      "[72,    28] loss: 2.404\n",
      "[72,    32] valid loss: 197.511\n",
      "[73,    28] loss: 2.396\n",
      "[73,    32] valid loss: 197.171\n",
      "[74,    28] loss: 2.388\n",
      "[74,    32] valid loss: 196.840\n",
      "[75,    28] loss: 2.380\n",
      "[75,    32] valid loss: 196.505\n",
      "[76,    28] loss: 2.372\n",
      "[76,    32] valid loss: 196.174\n",
      "[77,    28] loss: 2.364\n",
      "[77,    32] valid loss: 195.832\n",
      "[78,    28] loss: 2.356\n",
      "[78,    32] valid loss: 195.487\n",
      "[79,    28] loss: 2.348\n",
      "[79,    32] valid loss: 195.111\n",
      "[80,    28] loss: 2.341\n",
      "[80,    32] valid loss: 194.786\n",
      "[81,    28] loss: 2.333\n",
      "[81,    32] valid loss: 194.429\n",
      "[82,    28] loss: 2.325\n",
      "[82,    32] valid loss: 194.115\n",
      "[83,    28] loss: 2.318\n",
      "[83,    32] valid loss: 193.813\n",
      "[84,    28] loss: 2.311\n",
      "[84,    32] valid loss: 193.504\n",
      "[85,    28] loss: 2.304\n",
      "[85,    32] valid loss: 193.260\n",
      "[86,    28] loss: 2.297\n",
      "[86,    32] valid loss: 192.994\n",
      "[87,    28] loss: 2.291\n",
      "[87,    32] valid loss: 192.754\n",
      "[88,    28] loss: 2.284\n",
      "[88,    32] valid loss: 192.469\n",
      "[89,    28] loss: 2.278\n",
      "[89,    32] valid loss: 192.243\n",
      "[90,    28] loss: 2.272\n",
      "[90,    32] valid loss: 192.037\n",
      "[91,    28] loss: 2.266\n",
      "[91,    32] valid loss: 191.797\n",
      "[92,    28] loss: 2.261\n",
      "[92,    32] valid loss: 191.586\n",
      "[93,    28] loss: 2.255\n",
      "[93,    32] valid loss: 191.397\n",
      "[94,    28] loss: 2.250\n",
      "[94,    32] valid loss: 191.147\n",
      "[95,    28] loss: 2.244\n",
      "[95,    32] valid loss: 190.968\n",
      "[96,    28] loss: 2.239\n",
      "[96,    32] valid loss: 190.752\n",
      "[97,    28] loss: 2.234\n",
      "[97,    32] valid loss: 190.584\n",
      "[98,    28] loss: 2.228\n",
      "[98,    32] valid loss: 190.373\n",
      "[99,    28] loss: 2.223\n",
      "[99,    32] valid loss: 190.237\n",
      "[100,    28] loss: 2.218\n",
      "[100,    32] valid loss: 189.940\n",
      "[101,    28] loss: 2.213\n",
      "[101,    32] valid loss: 189.773\n",
      "[102,    28] loss: 2.208\n",
      "[102,    32] valid loss: 189.661\n",
      "[103,    28] loss: 2.203\n",
      "[103,    32] valid loss: 189.393\n",
      "[104,    28] loss: 2.198\n",
      "[104,    32] valid loss: 189.216\n",
      "[105,    28] loss: 2.192\n",
      "[105,    32] valid loss: 189.062\n",
      "[106,    28] loss: 2.188\n",
      "[106,    32] valid loss: 188.827\n",
      "[107,    28] loss: 2.183\n",
      "[107,    32] valid loss: 188.645\n",
      "[108,    28] loss: 2.178\n",
      "[108,    32] valid loss: 188.487\n",
      "[109,    28] loss: 2.173\n",
      "[109,    32] valid loss: 188.252\n",
      "[110,    28] loss: 2.168\n",
      "[110,    32] valid loss: 188.065\n",
      "[111,    28] loss: 2.162\n",
      "[111,    32] valid loss: 187.829\n",
      "[112,    28] loss: 2.157\n",
      "[112,    32] valid loss: 187.601\n",
      "[113,    28] loss: 2.152\n",
      "[113,    32] valid loss: 187.376\n",
      "[114,    28] loss: 2.147\n",
      "[114,    32] valid loss: 187.203\n",
      "[115,    28] loss: 2.142\n",
      "[115,    32] valid loss: 186.938\n",
      "[116,    28] loss: 2.135\n",
      "[116,    32] valid loss: 186.755\n",
      "[117,    28] loss: 2.129\n",
      "[117,    32] valid loss: 186.498\n",
      "[118,    28] loss: 2.124\n",
      "[118,    32] valid loss: 186.284\n",
      "[119,    28] loss: 2.119\n",
      "[119,    32] valid loss: 185.991\n",
      "[120,    28] loss: 2.113\n",
      "[120,    32] valid loss: 185.805\n",
      "[121,    28] loss: 2.105\n",
      "[121,    32] valid loss: 185.475\n",
      "[122,    28] loss: 2.099\n",
      "[122,    32] valid loss: 185.208\n",
      "[123,    28] loss: 2.093\n",
      "[123,    32] valid loss: 184.855\n",
      "[124,    28] loss: 2.087\n",
      "[124,    32] valid loss: 184.594\n",
      "[125,    28] loss: 2.080\n",
      "[125,    32] valid loss: 184.279\n",
      "[126,    28] loss: 2.073\n",
      "[126,    32] valid loss: 184.113\n",
      "[127,    28] loss: 2.065\n",
      "[127,    32] valid loss: 183.726\n",
      "[128,    28] loss: 2.058\n",
      "[128,    32] valid loss: 183.423\n",
      "[129,    28] loss: 2.051\n",
      "[129,    32] valid loss: 183.276\n",
      "[130,    28] loss: 2.044\n",
      "[130,    32] valid loss: 182.883\n",
      "[131,    28] loss: 2.037\n",
      "[131,    32] valid loss: 182.548\n",
      "[132,    28] loss: 2.029\n",
      "[132,    32] valid loss: 182.240\n",
      "[133,    28] loss: 2.022\n",
      "[133,    32] valid loss: 181.971\n",
      "[134,    28] loss: 2.015\n",
      "[134,    32] valid loss: 181.632\n",
      "[135,    28] loss: 2.008\n",
      "[135,    32] valid loss: 181.449\n",
      "[136,    28] loss: 2.000\n",
      "[136,    32] valid loss: 181.019\n",
      "[137,    28] loss: 1.992\n",
      "[137,    32] valid loss: 180.727\n",
      "[138,    28] loss: 1.985\n",
      "[138,    32] valid loss: 180.479\n",
      "[139,    28] loss: 1.979\n",
      "[139,    32] valid loss: 180.126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140,    28] loss: 1.973\n",
      "[140,    32] valid loss: 179.952\n",
      "[141,    28] loss: 1.965\n",
      "[141,    32] valid loss: 179.560\n",
      "[142,    28] loss: 1.959\n",
      "[142,    32] valid loss: 179.219\n",
      "[143,    28] loss: 1.952\n",
      "[143,    32] valid loss: 178.921\n",
      "[144,    28] loss: 1.945\n",
      "[144,    32] valid loss: 178.747\n",
      "[145,    28] loss: 1.939\n",
      "[145,    32] valid loss: 178.484\n",
      "[146,    28] loss: 1.934\n",
      "[146,    32] valid loss: 178.280\n",
      "[147,    28] loss: 1.926\n",
      "[147,    32] valid loss: 177.990\n",
      "[148,    28] loss: 1.922\n",
      "[148,    32] valid loss: 177.748\n",
      "[149,    28] loss: 1.915\n",
      "[149,    32] valid loss: 177.521\n",
      "[150,    28] loss: 1.909\n",
      "[150,    32] valid loss: 177.283\n",
      "[151,    28] loss: 1.904\n",
      "[151,    32] valid loss: 177.098\n",
      "[152,    28] loss: 1.899\n",
      "[152,    32] valid loss: 176.848\n",
      "[153,    28] loss: 1.892\n",
      "[153,    32] valid loss: 176.634\n",
      "[154,    28] loss: 1.887\n",
      "[154,    32] valid loss: 176.491\n",
      "[155,    28] loss: 1.882\n",
      "[155,    32] valid loss: 176.266\n",
      "[156,    28] loss: 1.878\n",
      "[156,    32] valid loss: 176.114\n",
      "[157,    28] loss: 1.872\n",
      "[157,    32] valid loss: 175.940\n",
      "[158,    28] loss: 1.867\n",
      "[158,    32] valid loss: 175.726\n",
      "[159,    28] loss: 1.863\n",
      "[159,    32] valid loss: 175.631\n",
      "[160,    28] loss: 1.859\n",
      "[160,    32] valid loss: 175.490\n",
      "[161,    28] loss: 1.854\n",
      "[161,    32] valid loss: 175.287\n",
      "[162,    28] loss: 1.849\n",
      "[162,    32] valid loss: 175.177\n",
      "[163,    28] loss: 1.846\n",
      "[163,    32] valid loss: 175.031\n",
      "[164,    28] loss: 1.841\n",
      "[164,    32] valid loss: 174.842\n",
      "[165,    28] loss: 1.837\n",
      "[165,    32] valid loss: 174.726\n",
      "[166,    28] loss: 1.834\n",
      "[166,    32] valid loss: 174.624\n",
      "[167,    28] loss: 1.830\n",
      "[167,    32] valid loss: 174.452\n",
      "[168,    28] loss: 1.826\n",
      "[168,    32] valid loss: 174.264\n",
      "[169,    28] loss: 1.822\n",
      "[169,    32] valid loss: 174.125\n",
      "[170,    28] loss: 1.819\n",
      "[170,    32] valid loss: 174.041\n",
      "[171,    28] loss: 1.815\n",
      "[171,    32] valid loss: 173.847\n",
      "[172,    28] loss: 1.812\n",
      "[172,    32] valid loss: 173.746\n",
      "[173,    28] loss: 1.808\n",
      "[173,    32] valid loss: 173.607\n",
      "[174,    28] loss: 1.805\n",
      "[174,    32] valid loss: 173.462\n",
      "[175,    28] loss: 1.801\n",
      "[175,    32] valid loss: 173.336\n",
      "[176,    28] loss: 1.799\n",
      "[176,    32] valid loss: 173.150\n",
      "[177,    28] loss: 1.795\n",
      "[177,    32] valid loss: 173.036\n",
      "[178,    28] loss: 1.792\n",
      "[178,    32] valid loss: 172.939\n",
      "[179,    28] loss: 1.788\n",
      "[179,    32] valid loss: 172.734\n",
      "[180,    28] loss: 1.785\n",
      "[180,    32] valid loss: 172.628\n",
      "[181,    28] loss: 1.782\n",
      "[181,    32] valid loss: 172.454\n",
      "[182,    28] loss: 1.779\n",
      "[182,    32] valid loss: 172.373\n",
      "[183,    28] loss: 1.775\n",
      "[183,    32] valid loss: 172.230\n",
      "[184,    28] loss: 1.772\n",
      "[184,    32] valid loss: 172.090\n",
      "[185,    28] loss: 1.768\n",
      "[185,    32] valid loss: 171.934\n",
      "[186,    28] loss: 1.766\n",
      "[186,    32] valid loss: 171.841\n",
      "[187,    28] loss: 1.762\n",
      "[187,    32] valid loss: 171.721\n",
      "[188,    28] loss: 1.758\n",
      "[188,    32] valid loss: 171.596\n",
      "[189,    28] loss: 1.755\n",
      "[189,    32] valid loss: 171.421\n",
      "[190,    28] loss: 1.751\n",
      "[190,    32] valid loss: 171.290\n",
      "[191,    28] loss: 1.749\n",
      "[191,    32] valid loss: 171.230\n",
      "[192,    28] loss: 1.745\n",
      "[192,    32] valid loss: 171.121\n",
      "[193,    28] loss: 1.742\n",
      "[193,    32] valid loss: 171.023\n",
      "[194,    28] loss: 1.739\n",
      "[194,    32] valid loss: 170.904\n",
      "[195,    28] loss: 1.736\n",
      "[195,    32] valid loss: 170.775\n",
      "[196,    28] loss: 1.733\n",
      "[196,    32] valid loss: 170.714\n",
      "[197,    28] loss: 1.730\n",
      "[197,    32] valid loss: 170.587\n",
      "[198,    28] loss: 1.726\n",
      "[198,    32] valid loss: 170.424\n",
      "[199,    28] loss: 1.724\n",
      "[199,    32] valid loss: 170.324\n",
      "[200,    28] loss: 1.722\n",
      "[200,    32] valid loss: 170.256\n",
      "[201,    28] loss: 1.718\n",
      "[201,    32] valid loss: 170.093\n",
      "[202,    28] loss: 1.715\n",
      "[202,    32] valid loss: 170.065\n",
      "[203,    28] loss: 1.712\n",
      "[203,    32] valid loss: 169.954\n",
      "[204,    28] loss: 1.709\n",
      "[204,    32] valid loss: 169.779\n",
      "[205,    28] loss: 1.707\n",
      "[205,    32] valid loss: 169.767\n",
      "[206,    28] loss: 1.703\n",
      "[206,    32] valid loss: 169.670\n",
      "[207,    28] loss: 1.701\n",
      "[207,    32] valid loss: 169.521\n",
      "[208,    28] loss: 1.697\n",
      "[208,    32] valid loss: 169.359\n",
      "[209,    28] loss: 1.695\n",
      "[209,    32] valid loss: 169.230\n",
      "[210,    28] loss: 1.692\n",
      "[210,    32] valid loss: 169.120\n",
      "[211,    28] loss: 1.689\n",
      "[211,    32] valid loss: 169.120\n",
      "[212,    28] loss: 1.686\n",
      "[212,    32] valid loss: 168.974\n",
      "[213,    28] loss: 1.684\n",
      "[213,    32] valid loss: 168.926\n",
      "[214,    28] loss: 1.681\n",
      "[214,    32] valid loss: 168.730\n",
      "[215,    28] loss: 1.679\n",
      "[215,    32] valid loss: 168.586\n",
      "[216,    28] loss: 1.675\n",
      "[216,    32] valid loss: 168.546\n",
      "[217,    28] loss: 1.672\n",
      "[217,    32] valid loss: 168.434\n",
      "[218,    28] loss: 1.670\n",
      "[218,    32] valid loss: 168.318\n",
      "[219,    28] loss: 1.667\n",
      "[219,    32] valid loss: 168.262\n",
      "[220,    28] loss: 1.665\n",
      "[220,    32] valid loss: 168.244\n",
      "[221,    28] loss: 1.662\n",
      "[221,    32] valid loss: 168.227\n",
      "[222,    28] loss: 1.660\n",
      "[222,    32] valid loss: 167.982\n",
      "[223,    28] loss: 1.657\n",
      "[223,    32] valid loss: 167.924\n",
      "[224,    28] loss: 1.655\n",
      "[224,    32] valid loss: 168.012\n",
      "[225,    28] loss: 1.652\n",
      "[225,    32] valid loss: 167.744\n",
      "[226,    28] loss: 1.651\n",
      "[226,    32] valid loss: 167.815\n",
      "[227,    28] loss: 1.648\n",
      "[227,    32] valid loss: 167.781\n",
      "[228,    28] loss: 1.646\n",
      "[228,    32] valid loss: 167.644\n",
      "[229,    28] loss: 1.644\n",
      "[229,    32] valid loss: 167.684\n",
      "[230,    28] loss: 1.641\n",
      "[230,    32] valid loss: 167.404\n",
      "[231,    28] loss: 1.639\n",
      "[231,    32] valid loss: 167.346\n",
      "[232,    28] loss: 1.638\n",
      "[232,    32] valid loss: 167.352\n",
      "[233,    28] loss: 1.635\n",
      "[233,    32] valid loss: 167.364\n",
      "[234,    28] loss: 1.633\n",
      "[234,    32] valid loss: 167.135\n",
      "[235,    28] loss: 1.631\n",
      "[235,    32] valid loss: 167.237\n",
      "[236,    28] loss: 1.628\n",
      "[236,    32] valid loss: 167.007\n",
      "[237,    28] loss: 1.626\n",
      "[237,    32] valid loss: 167.124\n",
      "[238,    28] loss: 1.624\n",
      "[238,    32] valid loss: 166.928\n",
      "[239,    28] loss: 1.622\n",
      "[239,    32] valid loss: 166.897\n",
      "[240,    28] loss: 1.620\n",
      "[240,    32] valid loss: 166.714\n",
      "[241,    28] loss: 1.618\n",
      "[241,    32] valid loss: 166.753\n",
      "[242,    28] loss: 1.616\n",
      "[242,    32] valid loss: 166.610\n",
      "[243,    28] loss: 1.614\n",
      "[243,    32] valid loss: 166.584\n",
      "[244,    28] loss: 1.612\n",
      "[244,    32] valid loss: 166.534\n",
      "[245,    28] loss: 1.610\n",
      "[245,    32] valid loss: 166.464\n",
      "[246,    28] loss: 1.609\n",
      "[246,    32] valid loss: 166.463\n",
      "[247,    28] loss: 1.607\n",
      "[247,    32] valid loss: 166.475\n",
      "[248,    28] loss: 1.604\n",
      "[248,    32] valid loss: 166.298\n",
      "[249,    28] loss: 1.603\n",
      "[249,    32] valid loss: 166.441\n",
      "[250,    28] loss: 1.601\n",
      "[250,    32] valid loss: 166.357\n",
      "[251,    28] loss: 1.599\n",
      "[251,    32] valid loss: 166.211\n",
      "[252,    28] loss: 1.597\n",
      "[252,    32] valid loss: 166.325\n",
      "[253,    28] loss: 1.596\n",
      "[253,    32] valid loss: 166.069\n",
      "[254,    28] loss: 1.593\n",
      "[254,    32] valid loss: 166.008\n",
      "[255,    28] loss: 1.591\n",
      "[255,    32] valid loss: 166.046\n",
      "[256,    28] loss: 1.590\n",
      "[256,    32] valid loss: 165.920\n",
      "[257,    28] loss: 1.588\n",
      "[257,    32] valid loss: 165.964\n",
      "[258,    28] loss: 1.586\n",
      "[258,    32] valid loss: 166.019\n",
      "[259,    28] loss: 1.585\n",
      "[259,    32] valid loss: 165.983\n",
      "[260,    28] loss: 1.583\n",
      "[260,    32] valid loss: 165.935\n",
      "[261,    28] loss: 1.581\n",
      "[261,    32] valid loss: 165.761\n",
      "[262,    28] loss: 1.580\n",
      "[262,    32] valid loss: 165.628\n",
      "[263,    28] loss: 1.578\n",
      "[263,    32] valid loss: 165.794\n",
      "[264,    28] loss: 1.576\n",
      "[264,    32] valid loss: 165.548\n",
      "[265,    28] loss: 1.575\n",
      "[265,    32] valid loss: 165.741\n",
      "[266,    28] loss: 1.573\n",
      "[266,    32] valid loss: 165.768\n",
      "[267,    28] loss: 1.571\n",
      "[267,    32] valid loss: 165.699\n",
      "[268,    28] loss: 1.570\n",
      "[268,    32] valid loss: 165.649\n",
      "[269,    28] loss: 1.568\n",
      "[269,    32] valid loss: 165.330\n",
      "[270,    28] loss: 1.567\n",
      "[270,    32] valid loss: 165.266\n",
      "[271,    28] loss: 1.565\n",
      "[271,    32] valid loss: 165.255\n",
      "[272,    28] loss: 1.563\n",
      "[272,    32] valid loss: 165.274\n",
      "[273,    28] loss: 1.563\n",
      "[273,    32] valid loss: 165.381\n",
      "[274,    28] loss: 1.561\n",
      "[274,    32] valid loss: 165.221\n",
      "[275,    28] loss: 1.560\n",
      "[275,    32] valid loss: 165.157\n",
      "[276,    28] loss: 1.558\n",
      "[276,    32] valid loss: 165.156\n",
      "[277,    28] loss: 1.556\n",
      "[277,    32] valid loss: 165.283\n",
      "[278,    28] loss: 1.554\n",
      "[278,    32] valid loss: 165.250\n",
      "[279,    28] loss: 1.553\n",
      "[279,    32] valid loss: 165.039\n",
      "[280,    28] loss: 1.552\n",
      "[280,    32] valid loss: 165.056\n",
      "[281,    28] loss: 1.551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[281,    32] valid loss: 164.921\n",
      "[282,    28] loss: 1.550\n",
      "[282,    32] valid loss: 164.932\n",
      "[283,    28] loss: 1.548\n",
      "[283,    32] valid loss: 164.921\n",
      "[284,    28] loss: 1.546\n",
      "[284,    32] valid loss: 164.897\n",
      "[285,    28] loss: 1.545\n",
      "[285,    32] valid loss: 164.797\n",
      "[286,    28] loss: 1.544\n",
      "[286,    32] valid loss: 164.796\n",
      "[287,    28] loss: 1.542\n",
      "[287,    32] valid loss: 164.956\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-2b3503ddb12c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrand\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mval_arr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-093fb7d66b4e>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmem_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mmemory_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                             \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmem_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmemory_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                         \u001b[0mtemporal_encoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmem_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmem_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_loss = []\n",
    "train_loss = []\n",
    "accuracy_test = []\n",
    "for rand in range(1):\n",
    "    net = network()\n",
    "    val_arr,loss_arr = trainer()\n",
    "    \n",
    "    val_loss.append(val_arr)\n",
    "    train_loss.append(loss_arr)\n",
    "    \n",
    "    accuracy_temp = []\n",
    "    for task in range(20):\n",
    "        accuracy_temp.append(accuracy_func(task))\n",
    "    \n",
    "    accuracy_test.append(accuracy_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot():\n",
    "    \n",
    "    plt.plot(np.arange(len(val_loss[0])),np.array(val_loss[0]),'-')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.show()\n",
    "    tr = []\n",
    "    for t in train_loss[0]:\n",
    "        tr.append(float(t[0].data))\n",
    "    plt.plot(np.arange(len(tr[1:])),np.array(tr[1:]),'r-')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.show()\n",
    "plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.706494140625"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(accuracy_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
